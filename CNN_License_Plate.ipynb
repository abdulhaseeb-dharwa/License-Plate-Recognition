{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "import kagglehub\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "6mV05DlXSRUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Download dataset\n",
        "print(\"Downloading dataset...\")\n",
        "dataset_path = kagglehub.dataset_download('fareselmenshawii/large-license-plate-dataset')\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "# Set paths\n",
        "train_images_path = os.path.join(dataset_path, 'images/train')\n",
        "train_labels_path = os.path.join(dataset_path, 'labels/train')\n",
        "val_images_path = os.path.join(dataset_path, 'images/val')\n",
        "val_labels_path = os.path.join(dataset_path, 'labels/val')\n",
        "test_images_path = os.path.join(dataset_path, 'images/test')\n",
        "test_labels_path = os.path.join(dataset_path, 'labels/test')\n"
      ],
      "metadata": {
        "id": "mIDqyDUiSTeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced data augmentation and transformation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((320, 320)),  # Larger input size\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((320, 320)),  # Larger input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class LicensePlateDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None, is_training=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.is_training = is_training\n",
        "\n",
        "        # Get all image files with .jpg extension\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
        "        print(f\"Found {len(self.image_files)} images in {image_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Get original dimensions for denormalization later\n",
        "        orig_width, orig_height = image.size\n",
        "\n",
        "        # Load label if it exists\n",
        "        label_name = os.path.splitext(img_name)[0] + '.txt'\n",
        "        label_path = os.path.join(self.label_dir, label_name)\n",
        "\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 5:  # Check if label format is valid\n",
        "                        # Extract YOLO format (class_id, x_center, y_center, width, height)\n",
        "                        # Ignore class_id\n",
        "                        x_center = float(parts[1])\n",
        "                        y_center = float(parts[2])\n",
        "                        width = float(parts[3])\n",
        "                        height = float(parts[4])\n",
        "\n",
        "                        # Convert to [x_min, y_min, x_max, y_max]\n",
        "                        x_min = x_center - width / 2\n",
        "                        y_min = y_center - height / 2\n",
        "                        x_max = x_center + width / 2\n",
        "                        y_max = y_center + height / 2\n",
        "\n",
        "                        # Clip values to ensure they are within [0, 1]\n",
        "                        x_min = max(0, min(1, x_min))\n",
        "                        y_min = max(0, min(1, y_min))\n",
        "                        x_max = max(0, min(1, x_max))\n",
        "                        y_max = max(0, min(1, y_max))\n",
        "\n",
        "                        boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        # If no boxes, create a dummy box (this shouldn't happen with this dataset)\n",
        "        if len(boxes) == 0:\n",
        "            boxes.append([0, 0, 0, 0])\n",
        "\n",
        "        # For simplicity and to ensure consistent batch sizes, use only the first box\n",
        "        if len(boxes) > 1:\n",
        "            boxes = [boxes[0]]\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert to tensor\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # Metadata for visualization and denormalization\n",
        "        metadata = {\n",
        "            'img_name': img_name,\n",
        "            'orig_width': orig_width,\n",
        "            'orig_height': orig_height\n",
        "        }\n",
        "\n",
        "        return image, boxes, metadata\n",
        "\n",
        "# Define Squeeze-and-Excitation Block\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "# Define ResNeXt-style Bottleneck Block with SE attention\n",
        "class SEResNeXtBlock(nn.Module):\n",
        "    expansion = 2  # Expansion factor for channels\n",
        "\n",
        "    def __init__(self, inplanes, planes, cardinality=32, stride=1, reduction=16):\n",
        "        super(SEResNeXtBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # Grouped convolution for ResNeXt style\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        # SE attention module\n",
        "        self.se = SEBlock(planes * self.expansion, reduction)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Skip connection with projection if needed\n",
        "        self.downsample = None\n",
        "        if stride != 1 or inplanes != planes * self.expansion:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        # Apply SE attention\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Feature Pyramid Network module for multi-scale feature extraction\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels):\n",
        "        super(FPN, self).__init__()\n",
        "\n",
        "        # Lateral connections (1x1 convolutions to reduce channel dimensions)\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "            for in_channels in in_channels_list\n",
        "        ])\n",
        "\n",
        "        # FPN connections (3x3 convolutions after upsampling)\n",
        "        self.fpn_convs = nn.ModuleList([\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "            for _ in range(len(in_channels_list))\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Last feature is the deepest with smallest resolution\n",
        "        last_inner = self.lateral_convs[-1](features[-1])\n",
        "        results = [self.fpn_convs[-1](last_inner)]\n",
        "\n",
        "        # Iterate from the second last layer\n",
        "        for idx in range(len(features) - 2, -1, -1):\n",
        "            # 1x1 conv on the current feature\n",
        "            lateral = self.lateral_convs[idx](features[idx])\n",
        "\n",
        "            # Upsample the deeper feature and add it to the current one\n",
        "            feat_size = lateral.size()[-2:]\n",
        "            inner_top_down = F.interpolate(results[0], size=feat_size, mode='nearest')\n",
        "            inner = lateral + inner_top_down\n",
        "\n",
        "            # Apply 3x3 conv and add to results\n",
        "            results.insert(0, self.fpn_convs[idx](inner))\n",
        "\n",
        "        return results\n",
        "\n",
        "# Improved License Plate Detector with FPN and SE-ResNeXt blocks\n",
        "class ImprovedLicensePlateDetector(nn.Module):\n",
        "    def __init__(self, cardinality=32):\n",
        "        super(ImprovedLicensePlateDetector, self).__init__()\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # SE-ResNeXt backbone layers\n",
        "        self.layer1 = self._make_layer(64, 64, blocks=3, cardinality=cardinality, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 128, blocks=4, cardinality=cardinality, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 256, blocks=6, cardinality=cardinality, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 512, blocks=3, cardinality=cardinality, stride=2)\n",
        "\n",
        "        # Feature Pyramid Network\n",
        "        in_channels_list = [128, 256, 512, 1024]  # After each layer (considering expansion)\n",
        "        self.fpn = FPN(in_channels_list, out_channels=256)\n",
        "\n",
        "        # Bounding box regression head\n",
        "        self.bbox_head = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        # Final linear layer\n",
        "        self.fc = nn.Linear(256, 4)\n",
        "\n",
        "        # Weight initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, inplanes, planes, blocks, cardinality, stride=1):\n",
        "        layers = []\n",
        "        # First block may have stride > 1\n",
        "        layers.append(SEResNeXtBlock(inplanes, planes, cardinality, stride))\n",
        "\n",
        "        # Rest of the blocks have stride = 1\n",
        "        inplanes = planes * SEResNeXtBlock.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(SEResNeXtBlock(inplanes, planes, cardinality))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:  # Check if bias exists before initializing\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:  # Check if bias exists before initializing\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Backbone feature extraction\n",
        "        c1 = self.layer1(x)\n",
        "        c2 = self.layer2(c1)\n",
        "        c3 = self.layer3(c2)\n",
        "        c4 = self.layer4(c3)\n",
        "\n",
        "        # FPN feature fusion\n",
        "        fpn_features = self.fpn([c1, c2, c3, c4])\n",
        "\n",
        "        # Use the most semantic feature map (P5) for bounding box regression\n",
        "        p5 = fpn_features[-1]\n",
        "        out = self.bbox_head(p5)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # Apply sigmoid to constrain outputs to [0, 1] range\n",
        "        out = torch.sigmoid(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Enhanced IoU Loss for better optimization\n",
        "class IoULoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Extract coordinates\n",
        "        pred_x1, pred_y1, pred_x2, pred_y2 = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]\n",
        "        target_x1, target_y1, target_x2, target_y2 = target[:, 0], target[:, 1], target[:, 2], target[:, 3]\n",
        "\n",
        "        # Calculate areas\n",
        "        pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
        "        target_area = (target_x2 - target_x1) * (target_y2 - target_y1)\n",
        "\n",
        "        # Calculate intersection coordinates\n",
        "        inter_x1 = torch.max(pred_x1, target_x1)\n",
        "        inter_y1 = torch.max(pred_y1, target_y1)\n",
        "        inter_x2 = torch.min(pred_x2, target_x2)\n",
        "        inter_y2 = torch.min(pred_y2, target_y2)\n",
        "\n",
        "        # Calculate intersection area\n",
        "        inter_w = torch.clamp(inter_x2 - inter_x1, min=0)\n",
        "        inter_h = torch.clamp(inter_y2 - inter_y1, min=0)\n",
        "        intersection = inter_w * inter_h\n",
        "\n",
        "        # Calculate union area\n",
        "        union = pred_area + target_area - intersection\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = intersection / (union + 1e-6)\n",
        "\n",
        "        # Return loss as (1 - IoU)\n",
        "        return 1 - iou.mean()\n",
        "\n",
        "# Combined loss for better optimization\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.iou_loss = IoULoss()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # IoU loss component\n",
        "        iou_loss = self.iou_loss(pred, target)\n",
        "\n",
        "        # MSE loss component\n",
        "        mse_loss = self.mse_loss(pred, target)\n",
        "\n",
        "        # Combined loss\n",
        "        return self.alpha * iou_loss + (1 - self.alpha) * mse_loss\n",
        "\n",
        "# Calculate IoU for evaluation\n",
        "def calculate_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Calculate IoU between boxes1 and boxes2\n",
        "\n",
        "    Args:\n",
        "        boxes1: tensor of shape [batch_size, 4] - [x_min, y_min, x_max, y_max]\n",
        "        boxes2: tensor of shape [batch_size, 4] - [x_min, y_min, x_max, y_max]\n",
        "\n",
        "    Returns:\n",
        "        IoU: tensor of shape [batch_size]\n",
        "    \"\"\"\n",
        "    # Extract coordinates\n",
        "    x1_min, y1_min, x1_max, y1_max = boxes1[:, 0], boxes1[:, 1], boxes1[:, 2], boxes1[:, 3]\n",
        "    x2_min, y2_min, x2_max, y2_max = boxes2[:, 0], boxes2[:, 1], boxes2[:, 2], boxes2[:, 3]\n",
        "\n",
        "    # Calculate areas\n",
        "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
        "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
        "\n",
        "    # Calculate intersection coordinates\n",
        "    inter_x_min = torch.max(x1_min, x2_min)\n",
        "    inter_y_min = torch.max(y1_min, y2_min)\n",
        "    inter_x_max = torch.min(x1_max, x2_max)\n",
        "    inter_y_max = torch.min(y1_max, y2_max)\n",
        "\n",
        "    # Calculate intersection area\n",
        "    inter_width = torch.clamp(inter_x_max - inter_x_min, min=0)\n",
        "    inter_height = torch.clamp(inter_y_max - inter_y_min, min=0)\n",
        "    inter_area = inter_width * inter_height\n",
        "\n",
        "    # Calculate union area\n",
        "    union_area = area1 + area2 - inter_area\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = inter_area / (union_area + 1e-6)  # Add small epsilon to avoid division by zero\n",
        "\n",
        "    return iou\n",
        "\n",
        "# Training function with additional validation checks\n",
        "def train(model, train_loader, optimizer, criterion, epoch, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_iou = 0.0\n",
        "\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} Train\")\n",
        "\n",
        "    for batch_idx, (images, targets, _) in progress_bar:\n",
        "        # We only have one box per image after our Dataset modification\n",
        "        # Reshape targets to match model output format [batch_size, 4]\n",
        "        boxes = targets.squeeze(1)  # This removes the second dimension which is now always 1\n",
        "\n",
        "        images = images.to(device)\n",
        "        boxes = boxes.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, boxes)\n",
        "\n",
        "        # Check for NaN losses and skip if found\n",
        "        if torch.isnan(loss).any():\n",
        "            print(f\"Warning: NaN loss detected in batch {batch_idx}. Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate IoU for this batch\n",
        "        with torch.no_grad():\n",
        "            batch_iou = calculate_iou(outputs, boxes).mean().item()\n",
        "            running_iou += batch_iou\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': running_loss / (batch_idx + 1),\n",
        "            'iou': running_iou / (batch_idx + 1)\n",
        "        })\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_iou = running_iou / len(train_loader)\n",
        "\n",
        "    # Step the scheduler if provided\n",
        "    if scheduler is not None:\n",
        "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            scheduler.step(epoch_loss)\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "    return epoch_loss, epoch_iou\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_iou = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n",
        "\n",
        "        for batch_idx, (images, targets, _) in progress_bar:\n",
        "            # We only have one box per image after our Dataset modification\n",
        "            # Reshape targets to match model output format [batch_size, 4]\n",
        "            boxes = targets.squeeze(1)  # This removes the second dimension which is now always 1\n",
        "\n",
        "            images = images.to(device)\n",
        "            boxes = boxes.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, boxes)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate IoU for this batch\n",
        "            batch_iou = calculate_iou(outputs, boxes).mean().item()\n",
        "            running_iou += batch_iou\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': running_loss / (batch_idx + 1),\n",
        "                'iou': running_iou / (batch_idx + 1)\n",
        "            })\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_iou = running_iou / len(val_loader)\n",
        "\n",
        "    return val_loss, val_iou\n",
        "\n",
        "# Inference function to visualize predictions\n",
        "def inference(model, test_loader, num_samples=5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Inference\")\n",
        "\n",
        "        for batch_idx, (images, targets, metadata) in progress_bar:\n",
        "            # Process only the specified number of samples\n",
        "            if batch_idx >= num_samples and num_samples > 0:\n",
        "                break\n",
        "\n",
        "            # For simplicity, process one image at a time\n",
        "            for i in range(images.size(0)):\n",
        "                if len(results) >= num_samples and num_samples > 0:\n",
        "                    break\n",
        "\n",
        "                image = images[i].unsqueeze(0).to(device)\n",
        "\n",
        "                # Get ground truth box for this image - with our dataset modification we have one box\n",
        "                gt_box = targets[i].squeeze().cpu().numpy()\n",
        "\n",
        "                # Get metadata for this image\n",
        "                img_name = metadata['img_name'][i]\n",
        "                orig_width = metadata['orig_width'][i]\n",
        "                orig_height = metadata['orig_height'][i]\n",
        "\n",
        "                # Get prediction\n",
        "                pred_box = model(image).squeeze().cpu().numpy()\n",
        "\n",
        "                # Calculate IoU\n",
        "                iou = calculate_iou(\n",
        "                    torch.tensor(pred_box).unsqueeze(0),\n",
        "                    torch.tensor(gt_box).unsqueeze(0)\n",
        "                ).item()\n",
        "\n",
        "                # Store result\n",
        "                results.append({\n",
        "                    'img_name': img_name,\n",
        "                    'orig_width': orig_width,\n",
        "                    'orig_height': orig_height,\n",
        "                    'gt_box': gt_box,\n",
        "                    'pred_box': pred_box,\n",
        "                    'iou': iou\n",
        "                })\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nInference Results:\")\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"\\nImage {i+1}: {result['img_name']}\")\n",
        "        print(f\"Original dimensions: {result['orig_width']}x{result['orig_height']}\")\n",
        "        print(f\"Ground truth box (normalized): {result['gt_box']}\")\n",
        "        print(f\"Predicted box (normalized): {result['pred_box']}\")\n",
        "        print(f\"IoU: {result['iou']:.4f}\")\n",
        "\n",
        "        # Convert normalized boxes to pixel coordinates for visualization\n",
        "        gt_box_pixels = [\n",
        "            result['gt_box'][0] * 320,\n",
        "            result['gt_box'][1] * 320,\n",
        "            result['gt_box'][2] * 320,\n",
        "            result['gt_box'][3] * 320\n",
        "        ]\n",
        "\n",
        "        pred_box_pixels = [\n",
        "            result['pred_box'][0] * 320,\n",
        "            result['pred_box'][1] * 320,\n",
        "            result['pred_box'][2] * 320,\n",
        "            result['pred_box'][3] * 320\n",
        "        ]\n",
        "\n",
        "        print(f\"Ground truth box (pixels): {gt_box_pixels}\")\n",
        "        print(f\"Predicted box (pixels): {pred_box_pixels}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Visualize predictions\n",
        "def visualize_predictions(results, test_loader):\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        for i, result in enumerate(results):\n",
        "            # Find the image in the dataset\n",
        "            for j, (images, targets, metadata) in enumerate(test_loader):\n",
        "                for k in range(images.size(0)):\n",
        "                    if metadata['img_name'][k] == result['img_name']:\n",
        "                        # Get the image and denormalize it\n",
        "                        image = images[k].permute(1, 2, 0).cpu().numpy()\n",
        "                        image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "                        image = np.clip(image, 0, 1)\n",
        "\n",
        "                        # Create a figure\n",
        "                        plt.figure(figsize=(10, 10))\n",
        "                        plt.imshow(image)\n",
        "\n",
        "                        # Draw ground truth box\n",
        "                        gt_box = result['gt_box']\n",
        "                        x_min, y_min, x_max, y_max = gt_box[0], gt_box[1], gt_box[2], gt_box[3]\n",
        "                        width = x_max - x_min\n",
        "                        height = y_max - y_min\n",
        "                        plt.gca().add_patch(plt.Rectangle((x_min * 320, y_min * 320),\n",
        "                                                        width * 320, height * 320,\n",
        "                                                        fill=False, edgecolor='g', linewidth=2))\n",
        "\n",
        "                        # Draw predicted box\n",
        "                        pred_box = result['pred_box']\n",
        "                        x_min, y_min, x_max, y_max = pred_box[0], pred_box[1], pred_box[2], pred_box[3]\n",
        "                        width = x_max - x_min\n",
        "                        height = y_max - y_min\n",
        "                        plt.gca().add_patch(plt.Rectangle((x_min * 320, y_min * 320),\n",
        "                                                        width * 320, height * 320,\n",
        "                                                        fill=False, edgecolor='r', linewidth=2))\n",
        "\n",
        "                        plt.title(f\"Image: {result['img_name']}, IoU: {result['iou']:.4f}\")\n",
        "                        plt.legend(['Ground Truth', 'Prediction'])\n",
        "                        plt.show()\n",
        "                        break\n",
        "                else:\n",
        "                    continue\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visualization: {e}\")\n",
        "        print(\"Skipping visualization. This doesn't affect model training/evaluation.\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Create datasets with different transforms for training and validation\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = LicensePlateDataset(train_images_path, train_labels_path, transform=transform_train, is_training=True)\n",
        "    val_dataset = LicensePlateDataset(val_images_path, val_labels_path, transform=transform_val)\n",
        "    test_dataset = LicensePlateDataset(test_images_path, test_labels_path, transform=transform_val)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 16  # Smaller batch size for more iterations and stable convergence\n",
        "    # Reduce num_workers if running into memory issues\n",
        "    num_workers = 2 if torch.cuda.is_available() else 0\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing improved model...\")\n",
        "    model = ImprovedLicensePlateDetector(cardinality=16).to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = CombinedLoss(alpha=0.7)  # Higher weight to IoU loss\n",
        "\n",
        "    # Define optimizer with weight decay\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "    # Learning rate scheduler with warmup\n",
        "    num_epochs = 20\n",
        "    warmup_epochs = 2\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < warmup_epochs:\n",
        "            # Linear warmup\n",
        "            return (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            # Cosine annealing\n",
        "            return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))\n",
        "\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "    # Initialize best model state and metrics\n",
        "    best_model_state = None\n",
        "    best_val_iou = 0.0\n",
        "\n",
        "    # Lists to store metrics for plotting\n",
        "    train_losses = []\n",
        "    train_ious = []\n",
        "    val_losses = []\n",
        "    val_ious = []\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train for one epoch\n",
        "        epoch_loss, epoch_iou = train(model, train_loader, optimizer, criterion, epoch, scheduler)\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_ious.append(epoch_iou)\n",
        "\n",
        "        # Validate model\n",
        "        val_loss, val_iou = validate(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        val_ious.append(val_iou)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train IoU: {epoch_iou:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_iou > best_val_iou:\n",
        "            best_val_iou = val_iou\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"  ↳ New best model saved with validation IoU: {best_val_iou:.4f}\")\n",
        "\n",
        "            # Save checkpoint\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_iou': val_iou,\n",
        "                'val_loss': val_loss,\n",
        "            }\n",
        "            torch.save(checkpoint, 'best_license_plate_detector.pth')\n",
        "\n",
        "    # Plot training and validation metrics\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot IoUs\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_ious, label='Train IoU')\n",
        "    plt.plot(val_ious, label='Validation IoU')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('IoU')\n",
        "    plt.title('Training and Validation IoU')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Final validation\n",
        "    final_val_loss, final_val_iou = validate(model, val_loader, criterion)\n",
        "    print(f\"\\nFinal Validation - Loss: {final_val_loss:.4f}, IoU: {final_val_iou:.4f}\")\n",
        "\n",
        "    # Run inference on test set\n",
        "    print(\"\\nRunning inference on test set...\")\n",
        "    results = inference(model, test_loader, num_samples=5)\n",
        "\n",
        "    # Visualize predictions\n",
        "    print(\"\\nVisualizing predictions...\")\n",
        "    visualize_predictions(results, test_loader)\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "\n",
        "    return model, results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olkEYnzTnJK2",
        "outputId": "37a05ad3-c1f8-456a-f94e-58ede5b56817"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Downloading dataset...\n",
            "Dataset downloaded to: /kaggle/input/large-license-plate-dataset\n",
            "Loading datasets...\n",
            "Found 25470 images in /kaggle/input/large-license-plate-dataset/images/train\n",
            "Found 1073 images in /kaggle/input/large-license-plate-dataset/images/val\n",
            "Found 386 images in /kaggle/input/large-license-plate-dataset/images/test\n",
            "Initializing improved model...\n",
            "Starting training for 20 epochs...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Train: 100%|██████████| 1592/1592 [09:51<00:00,  2.69it/s, loss=0.61, iou=0.153]\n",
            "Validation: 100%|██████████| 68/68 [00:19<00:00,  3.43it/s, loss=0.681, iou=0.0538]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Train Loss: 0.6103, Train IoU: 0.1531, Val Loss: 0.6811, Val IoU: 0.0538\n",
            "  ↳ New best model saved with validation IoU: 0.0538\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 Train: 100%|██████████| 1592/1592 [09:23<00:00,  2.82it/s, loss=0.516, iou=0.279]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.92it/s, loss=0.673, iou=0.0669]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20 - Train Loss: 0.5157, Train IoU: 0.2785, Val Loss: 0.6735, Val IoU: 0.0669\n",
            "  ↳ New best model saved with validation IoU: 0.0669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Train: 100%|██████████| 1592/1592 [09:21<00:00,  2.83it/s, loss=0.453, iou=0.367]\n",
            "Validation: 100%|██████████| 68/68 [00:18<00:00,  3.65it/s, loss=0.648, iou=0.102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Train Loss: 0.4529, Train IoU: 0.3667, Val Loss: 0.6480, Val IoU: 0.1017\n",
            "  ↳ New best model saved with validation IoU: 0.1017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Train: 100%|██████████| 1592/1592 [09:21<00:00,  2.84it/s, loss=0.421, iou=0.411]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.84it/s, loss=0.627, iou=0.128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 - Train Loss: 0.4215, Train IoU: 0.4110, Val Loss: 0.6268, Val IoU: 0.1282\n",
            "  ↳ New best model saved with validation IoU: 0.1282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Train: 100%|██████████| 1592/1592 [09:22<00:00,  2.83it/s, loss=0.398, iou=0.444]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.87it/s, loss=0.623, iou=0.151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 - Train Loss: 0.3983, Train IoU: 0.4435, Val Loss: 0.6227, Val IoU: 0.1511\n",
            "  ↳ New best model saved with validation IoU: 0.1511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Train: 100%|██████████| 1592/1592 [09:16<00:00,  2.86it/s, loss=0.386, iou=0.462]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.89it/s, loss=0.629, iou=0.145]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 - Train Loss: 0.3856, Train IoU: 0.4616, Val Loss: 0.6288, Val IoU: 0.1449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7 Train: 100%|██████████| 1592/1592 [09:19<00:00,  2.85it/s, loss=0.368, iou=0.486]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.94it/s, loss=0.588, iou=0.191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 - Train Loss: 0.3684, Train IoU: 0.4857, Val Loss: 0.5879, Val IoU: 0.1906\n",
            "  ↳ New best model saved with validation IoU: 0.1906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Train: 100%|██████████| 1592/1592 [09:23<00:00,  2.82it/s, loss=0.357, iou=0.502]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.94it/s, loss=0.614, iou=0.156]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 - Train Loss: 0.3569, Train IoU: 0.5019, Val Loss: 0.6136, Val IoU: 0.1562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9 Train: 100%|██████████| 1592/1592 [09:15<00:00,  2.87it/s, loss=0.355, iou=0.504]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.88it/s, loss=0.568, iou=0.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 - Train Loss: 0.3554, Train IoU: 0.5041, Val Loss: 0.5680, Val IoU: 0.2098\n",
            "  ↳ New best model saved with validation IoU: 0.2098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Train: 100%|██████████| 1592/1592 [09:17<00:00,  2.86it/s, loss=0.348, iou=0.513]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.89it/s, loss=0.558, iou=0.221]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 - Train Loss: 0.3483, Train IoU: 0.5133, Val Loss: 0.5580, Val IoU: 0.2210\n",
            "  ↳ New best model saved with validation IoU: 0.2210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Train: 100%|██████████| 1592/1592 [09:16<00:00,  2.86it/s, loss=0.34, iou=0.525]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.81it/s, loss=0.561, iou=0.218]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 - Train Loss: 0.3400, Train IoU: 0.5250, Val Loss: 0.5609, Val IoU: 0.2178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12 Train: 100%|██████████| 1592/1592 [09:19<00:00,  2.84it/s, loss=0.332, iou=0.537]\n",
            "Validation: 100%|██████████| 68/68 [00:17<00:00,  3.91it/s, loss=0.56, iou=0.223]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 - Train Loss: 0.3316, Train IoU: 0.5370, Val Loss: 0.5596, Val IoU: 0.2231\n",
            "  ↳ New best model saved with validation IoU: 0.2231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Train: 100%|██████████| 1592/1592 [09:22<00:00,  2.83it/s, loss=0.325, iou=0.546]\n",
            "Validation: 100%|██████████| 68/68 [00:18<00:00,  3.62it/s, loss=0.55, iou=0.231]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 - Train Loss: 0.3251, Train IoU: 0.5462, Val Loss: 0.5500, Val IoU: 0.2310\n",
            "  ↳ New best model saved with validation IoU: 0.2310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Train:  26%|██▌       | 408/1592 [02:24<06:51,  2.88it/s, loss=0.321, iou=0.552]"
          ]
        }
      ]
    }
  ]
}